---
name: ai-code-reviewer
description: Comprehensive code review system with AI agents for analyzing implementations created by other AI agents. Use this skill when you need to review, validate, or critique code generated by AI agents (including yourself or other AI systems). Triggers include requests to review AI-generated code, validate implementations, perform quality assurance on automated code, check AI agent outputs, or conduct peer reviews of code from other AI systems. Also use for second-pass reviews, implementation audits, or when users explicitly ask to review code that was generated by an AI or automated system.
---

# AI Code Reviewer Skill

This skill enables comprehensive code review of implementations created by AI agents, providing structured analysis, quality validation, and actionable feedback.

## Overview

Code review process involves five sequential phases:

1. **Intake & Context** - Understand what was implemented and why
2. **Static Analysis** - Automated checks for common issues
3. **Deep Review** - Manual inspection across multiple dimensions
4. **Testing Validation** - Verify functionality and edge cases
5. **Report Generation** - Deliver structured, actionable feedback

## Phase 1: Intake & Context

### Gather Implementation Details

Before reviewing, collect:

**Required:**
- The code implementation (files, snippets, or repository)
- Original requirements or user request
- Programming language(s) and framework(s) used

**Helpful if available:**
- AI agent's reasoning or implementation notes
- Expected behavior or acceptance criteria
- Known constraints or limitations
- Related documentation

### Load Review Checklist

Read the appropriate language-specific checklist from `references/`:

```bash
view references/python-checklist.md      # For Python code
view references/javascript-checklist.md  # For JavaScript/TypeScript
view references/java-checklist.md        # For Java code
view references/php-checklist.md         # For PHP code
view references/general-checklist.md     # For other languages
```

If the specific language checklist doesn't exist, use `general-checklist.md`.

## Phase 2: Static Analysis

Run automated checks using the validation script:

```bash
python scripts/static_analyzer.py <code_directory> --language <lang> --output /home/claude/analysis_report.json
```

Parameters:
- `<code_directory>`: Path to code files
- `--language`: python, javascript, java, cpp, go, etc.
- `--output`: Where to save analysis results

The script checks:
- Syntax errors and warnings
- Code style violations
- Security vulnerabilities (basic patterns)
- Complexity metrics
- Dependency issues

Review the JSON output before proceeding.

## Phase 3: Deep Review

Conduct manual review across seven dimensions:

### 1. Correctness & Logic

**Check:**
- Does code implement the stated requirements?
- Are edge cases handled properly?
- Is the logic sound and free of bugs?
- Are there off-by-one errors, race conditions, or logic flaws?

**Red flags:**
- Hardcoded values that should be configurable
- Missing null/undefined checks
- Incorrect loop boundaries
- Unhandled error states

### 2. Code Quality & Maintainability

**Check:**
- Is code readable and well-structured?
- Are functions/classes appropriately sized?
- Is naming clear and consistent?
- Is there unnecessary complexity?

**Red flags:**
- Functions longer than 50 lines (guideline, not rule)
- Deeply nested logic (>4 levels)
- Cryptic variable names (x, tmp, data)
- Duplicate code blocks

### 3. Security & Safety

**Check:**
- Are inputs validated and sanitized?
- Are there SQL injection, XSS, or CSRF vulnerabilities?
- Are secrets/credentials properly managed?
- Are file operations safe (path traversal, etc.)?

**Critical issues:**
- Direct execution of user input
- Hardcoded credentials or API keys
- Missing authentication/authorization
- Unsafe deserialization

### 4. Performance & Efficiency

**Check:**
- Are algorithms appropriately efficient?
- Are there unnecessary loops or operations?
- Is memory usage reasonable?
- Are database queries optimized?

**Red flags:**
- O(n¬≤) or worse when O(n) is possible
- Fetching all records when pagination available
- Multiple database queries in loops (N+1 problem)
- Large objects loaded into memory unnecessarily

### 5. Error Handling & Resilience

**Check:**
- Are exceptions caught and handled appropriately?
- Are error messages informative?
- Is there proper logging?
- Can the system recover from failures?

**Red flags:**
- Empty catch blocks
- Generic exception catching without re-throwing
- No logging of errors
- Silent failures

### 6. Testing & Testability

**Check:**
- Is the code testable (no tight coupling)?
- Are edge cases considered?
- Are dependencies injectable?
- Is mocking possible where needed?

**Red flags:**
- Static dependencies that can't be mocked
- Global state mutations
- Time-dependent logic without abstraction
- Tight coupling to external services

### 7. AI-Specific Concerns

**Check:**
- Did the AI understand the requirements correctly?
- Are there signs of hallucinated APIs or libraries?
- Is the implementation pattern-appropriate for the language?
- Are there unnecessary complications (AI overthinking)?

**Red flags:**
- Use of non-existent functions or libraries
- Overly complex solutions to simple problems
- Mixing incompatible library versions
- Outdated or deprecated patterns

## Phase 4: Testing Validation

### Execute Tests

If tests are provided, run them:

```bash
python scripts/test_runner.py <code_directory> --framework <pytest|unittest|jest|junit>
```

### Create Test Cases

If tests are missing, create basic validation:

```python
# Generate test template
python scripts/generate_tests.py <code_file> --output <test_file>
```

Then review and enhance the generated tests.

### Verify Edge Cases

Manually check critical edge cases:
- Null/undefined inputs
- Empty collections
- Boundary values (0, -1, max values)
- Concurrent access (if applicable)
- Network failures (if applicable)

## Phase 5: Report Generation

Generate structured feedback using the report template:

```bash
python scripts/generate_report.py \
  --static-analysis analysis_report.json \
  --manual-review review_notes.md \
  --output code_review_report.md
```

### Report Structure

The report follows this template (see `references/report-template.md`):

```markdown
# Code Review Report

## Executive Summary
[2-3 sentence overview: overall quality, critical issues, recommendation]

## Critical Issues üö®
[Must-fix problems that block deployment]

## Major Concerns ‚ö†Ô∏è
[Important issues that should be addressed]

## Minor Suggestions üí°
[Nice-to-have improvements]

## Positive Observations ‚úÖ
[What the AI agent did well]

## Detailed Findings
[Organized by the seven review dimensions]

## Recommendations
[Specific, actionable next steps]

## Approval Status
- [ ] Approved - Ready to use
- [ ] Approved with minor changes
- [ ] Requires revision
- [ ] Requires major refactoring
```

### Severity Guidelines

**Critical (üö®):**
- Security vulnerabilities
- Data loss risks
- Incorrect core logic
- Code that will crash in production

**Major (‚ö†Ô∏è):**
- Performance issues affecting user experience
- Poor error handling
- Significant maintainability concerns
- Missing important features

**Minor (üí°):**
- Code style inconsistencies
- Opportunities for simplification
- Documentation improvements
- Non-critical optimizations

## Quality Standards

Apply these standards when reviewing:

**Code must:**
- Actually work (solve the stated problem)
- Be secure (no critical vulnerabilities)
- Handle errors gracefully
- Be reasonably efficient

**Code should:**
- Follow language conventions
- Be readable and maintainable
- Include appropriate comments
- Have descriptive names

**Code could:**
- Be more elegant or concise
- Use more advanced features
- Have better documentation
- Be more performant (if not critical)

Balance thoroughness with pragmatism. Not every issue needs fixing‚Äîprioritize based on impact and effort.

## Common AI Agent Pitfalls

Watch for these typical issues in AI-generated code:

1. **Hallucinated APIs**: Non-existent functions or incorrect signatures
2. **Outdated patterns**: Code using deprecated methods
3. **Over-engineering**: Complex solutions to simple problems
4. **Missing context**: Code that doesn't integrate well with existing systems
5. **Incomplete error handling**: Happy path only
6. **Copy-paste inconsistency**: Variable names or logic from examples that don't apply
7. **Library version mismatches**: Mixing incompatible versions
8. **Misunderstood requirements**: Solving the wrong problem

## Best Practices

**Do:**
- Start with static analysis before manual review
- Be specific in feedback (include line numbers, examples)
- Explain WHY something is an issue, not just WHAT
- Acknowledge what was done well
- Provide code examples for suggested fixes
- Consider the AI's likely training data when evaluating patterns

**Don't:**
- Nitpick style issues while ignoring logic bugs
- Assume malice‚ÄîAI agents don't have intent
- Demand perfect code when good code suffices
- Review without understanding requirements
- Focus only on what's wrong

## Output Expectations

Your final deliverable should include:

1. **Main Report**: Structured markdown document with findings
2. **Annotated Code**: Original code with inline comments on issues (optional but helpful)
3. **Fixed Code**: Corrected version for critical issues (if requested)
4. **Test Cases**: Missing tests you've created (if applicable)

## Advanced Features

### Comparative Review

When reviewing multiple AI implementations of the same task:

```bash
python scripts/compare_implementations.py \
  --impl1 agent_a/code.py \
  --impl2 agent_b/code.py \
  --output comparison_report.md
```

### Continuous Review

For iterative development with an AI agent:

1. Review initial implementation
2. Provide feedback
3. Review revised code
4. Track improvement across iterations

Document what was fixed and what remains in each iteration.

## Troubleshooting

**"Static analyzer fails"**
‚Üí Ensure code directory structure is correct and language is specified

**"Can't determine requirements"**
‚Üí Ask user for original prompt or intended behavior

**"Code uses unknown library"**
‚Üí Likely hallucinated‚Äîflag as critical issue

**"Not sure if edge case is handled"**
‚Üí Write a test to verify behavior

## References

Store all reports in `reports/` directory.

Detailed language-specific guidance available in:
- `references/python-checklist.md` - Python best practices
- `references/javascript-checklist.md` - JavaScript/TypeScript patterns
- `references/java-checklist.md` - Java conventions
- `references/general-checklist.md` - Universal principles
- `references/security-patterns.md` - Common vulnerabilities
- `references/report-template.md` - Full report template with examples
